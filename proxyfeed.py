#!/usr/bin/env python3

#######################################################################
# This program source is a part of a literate program and was produced
# by nuweb. Please see the accompanying literate program pdf for the
# full documentation.
#
# author: Michael J. Rossi
# contact: file13@hushmail.me
# literate pdf file: proxyfeed.pdf
#######################################################################

#######################################################################
# Copyright (c) 2015, Michael J. Rossi
# All rights reserved.
#
# Redistribution and use in source and binary forms, with or without
# modification, are permitted provided that the following conditions
# are met:
#
# * Redistributions of source code must retain the above copyright
# notice, this list of conditions and the following disclaimer.
#
# * Redistributions in binary form must reproduce the above copyright
# notice, this list of conditions and the following disclaimer in the
# documentation and/or other materials provided with the distribution.
#
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
# "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
# A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
# HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
# SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
# LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
# DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
# THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
#
#######################################################################


from urllib.request import urlopen, Request
from xml.etree.ElementTree import parse
from copy import copy, deepcopy
import datetime

# 3rd party libraries
from ipwhois import IPWhois
from termcolor import cprint

def extract_proxy_data(doc, namespace_spec_url):
    """Extracts the data from an already parsed (also use deepcopy) XML doc. Returns a list of dictionaries of five strings containing the 'ip', 'port', 'country', 'timestamp', and 'whois'."""
    namespaces = {'prx': namespace_spec_url}
    proxies = []
    try:
        for i in doc.iterfind('channel/item/prx:proxy', namespaces):
            ip = i.find('prx:ip', namespaces)
            port = i.find('prx:port', namespaces)
            timestamp = i.find('prx:check_timestamp', namespaces)
            if timestamp == None:  # make fault tolerant
                timestamp = copy(ip)
                timestamp.text = 'Unknown'
            country = i.find('prx:country', namespaces)
            if country == None:
                country = i.find('prx:country_code', namespaces)
            if country == None: 
                country = copy(ip)
                country.text = 'Unknown'
            proxy = {'ip':ip.text,
                     'port':port.text,
                     'country':country.text,
                     'timestamp':timestamp.text,
                     'whois':'Unknown'}
            proxies.append(proxy)
    except Exception as e:
        cprint('[-] Error processing XML feed %s!' % doc, 'red')
        cprint('[-] Error: %s' % e, 'yellow')
        quit()
    return proxies

def get_proxy_feed(url, verbose=False, agent='Mozilla/5.0'):
    """Given a url string, pull a proxy RSS feed conforming to the proxyrss.com specification and return a list of proxy dictionaries. Some proxy feeds apparently don't like Python, so we'll use a different user-agent."""
    request = Request(url, headers={'User-Agent': agent})
    try:
        u = urlopen(request)
    except Exception as e:
        cprint('[-] Error fetching feed %s!' % url, 'red')
        cprint('[-] Error: %s' % e, 'yellow')
        quit()
    doc = parse(u)
    spec = 'http://www.proxyrss.com/specification.html'
    proxies = extract_proxy_data(deepcopy(doc), spec)
    if len(proxies) == 0:  # if empty list, try the other XML spec
        spec = 'http://www.proxyrss.com/content'
        proxies = extract_proxy_data(deepcopy(doc), spec)
    if verbose:
        cprint('[+] Found %s total proxies.' % len(proxies), 'green')
    return proxies

def filter_by_country(proxy_list, country_code, country, verbose=False):
    """Remove any non-US entries from a proxy list generated by get_proxy_feed."""
    proxies = []
    for i in proxy_list:
        if i['country'].endswith(country_code) or i['country'].endswith(country):
            proxies.append(i)
    if verbose:
        cprint('[+] Found %s %s proxies.' %
               (len(proxies), country_code), 'green')
    return proxies

def get_whois_description(ip):
    """Get's the 'description' field from a whois entry given an IP string."""
    obj = IPWhois(ip)
    result = obj.lookup()
    return result['nets'][0]['description']

def lookup_whois(proxies, verbose=True):
    """Performs a whois lookup for the 'description' returning a new list with the 'whois' added to the dictionaries."""
    results = []
    count = 0
    for proxy in proxies:
        try:
            whois = get_whois_description(proxy['ip'])
            whois = whois.replace("\n", " ")  # strip newlines
            whois = whois.replace("\r", " ")
            proxy['whois'] = whois
            results.append(proxy)
            if verbose:
                count += 1
                if count % 5 == 0:
                    cprint('[+] Completed %s look ups.' % count,
                           'green')
        except Exception as e:
            cprint('[-] Error resolving whois for %s!' % proxy, 'red')
            cprint('[-] Error: %s' % e, 'yellow')
            cprint('[-] Skipping entry.', 'red')
            continue
    return results

def is_ip_in_dict(ip, proxies):
    """Predicate to test if an IP address string is found in a proxy dictionary. We need this since we're dealing with lists of dictionaries."""
    result = False
    for proxy in proxies:
        if ip == proxy['ip']:
            result = True
            break
    return result

def remove_duplicates(proxies):
    """Removes any duplicate IPs from our proxies list of lists."""
    results = []
    for proxy in proxies:
        if results == []:
            results.append(proxy)
            continue
        elif not is_ip_in_dict(proxy['ip'], results):
            results.append(proxy)
    return results

def convert_time(time_string):
    """Normalizes a date/time string into a format like so: 08-19-2015 at 13:54:28 UTC"""
    format = "%m-%d-%Y at %H:%M:00 UTC"
    if time_string == "Unknown":
        now = datetime.datetime.now()
        x = "Unknown time, listing found at "
        x += now.strftime(format)
        return x
    elif "/" in time_string:
        tstr = time_string.split()
        x = tstr[0].replace("/", "-")
        now = datetime.datetime.now()
        x += "-" + str(now.year) + " at " + tstr[1] + " UTC"
        return x
    else:
        dt = datetime.datetime.utcfromtimestamp(int(time_string))
        return dt.strftime(format)

def print_results(results, print_country=True, human_readable=True):
    """Prints out the results in either a 'human readable format' or as a grep friendly ':' separated strings. We can also optionally skip printing the country in the results."""
    if human_readable:
        for i in results:
            #print("Human: " + i['timestamp'])
            time = convert_time(i['timestamp'])
            #print("Human: " + time)
            cprint(i['ip'] + '|' + i['port'] + '|' + time, 'blue', end="")
            cprint("\t => ", 'yellow', end="")
            if print_country:
                cprint(i['country'], 'magenta', end="")
                cprint(" => ", 'yellow', end="")
            cprint(i['whois'], 'green')
    else:  # non-human readable
        for i in results:
            #print("Nonhuman: ", i['timestamp'])
            time = convert_time(i['timestamp'])
            #print("Nonhuman: " + time)
            print(i['ip'] + '|' + i['port'] + '|' + time, end="")
            print("|", end="")
            if print_country:
                print(i['country'], end="")
                print("|", end="")
            print(i['whois'])

def do_all(feeds, print_country=True, verbose=False,
           human_readable=False):
    """Look up and report the results for all proxies."""
    results = []
    for url in feeds:
        if verbose:
            print("[+] Fetching proxies from: %s" % url)
        results += get_proxy_feed(url, verbose)
    # remove the duplicate entries before doing slow whois
    results = remove_duplicates(results)
    if verbose:
        cprint('[+] Found %s unique proxies in feeds.' % len(results),
               'yellow')
        print('[+] Resolving whois descriptions: This can be slow...')
    final = lookup_whois(results, verbose)
    if verbose:
        cprint('[+] Total: %s proxies' % len(results), 'yellow')
    print_results(final, print_country, human_readable)

def do_country(feeds, print_country=False, verbose=False,
               human_readable=False,
               country_code='US', country='United States'):
    """Look up and report the results for country specific proxies."""
    results = []
    for url in feeds:
        if verbose:
            print("[+] Fetching proxies from: %s" % url)
        results += get_proxy_feed(url, verbose)
        results = filter_by_country(results, country_code, country, verbose)
    # remove the duplicate entries before doing slow whois
    results = remove_duplicates(results)
    if verbose:
        cprint('[+] Found %s unique proxies in feeds.' % len(results),
               'yellow')
        print('[+] Resolving whois descriptions: This can be slow...')
    final = lookup_whois(results, verbose)
    if verbose:
        cprint('[+] Total: %s %s proxies' % (len(results), country_code), 'yellow')
    print_results(final, print_country, human_readable)

if __name__ == '__main__':
    feeds = ["http://www.proxz.com/proxylists.xml",
             "http://www.freeproxylists.com/rss",
             "http://www.xroxy.com/proxyrss.xml"]

    
    import argparse
    parser = argparse.ArgumentParser(
        description="""Pull latest data from the open proxy RSS feeds.
        (file13@hushmail.me)""")
    parser.add_argument('-c', dest='country_specific',
                        metavar='country',
                        action='append',
                        help='filter by 1 country_code AND 1 country ie: -c "US" -c "United States"')
    parser.add_argument('-d', dest='print_country',
                        action='store_false',
                        help='do NOT print country in results')
    parser.add_argument('-g', dest='human_readable',
                        action='store_false',
                        help="grep parseable results (delimiter='|')")
    parser.add_argument('-l', dest='list_feeds',
                        action='store_true',
                        help='print the default proxy feeds list')
    parser.add_argument('-u', dest='us_mode',
                        action='store_true',
                        help='only report US proxies (implies -c)')
    parser.add_argument('-v', dest='verbose',
                        action='store_true',
                        help='verbose mode')
    args = parser.parse_args()                
    
    
    if args.list_feeds:
        print('Default open proxy RSS feeds:')
        for url in feeds:
            print(url)
        quit()
    if args.us_mode:
        do_country(feeds,
                   print_country=args.print_country,
                   verbose=args.verbose,
                   human_readable=args.human_readable)
    elif args.country_specific:
        if len(args.country_specific) != 2:
            parser.print_help()
        else:
            do_country(feeds,
                       print_country=args.print_country,
                       verbose=args.verbose,
                       human_readable=args.human_readable,
                       country_code=args.country_specific[0],
                       country=args.country_specific[1])
    else:
        do_all(feeds,
               print_country=args.print_country,
               verbose=args.verbose,
               human_readable=args.human_readable)
    

